{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"t6MPjfT5NrKQ"},"source":["<div align=\"center\">\n","\n","  <a href=\"https://github.com/leemjm92/STEAMxD\" target=\"_blank\">\n","    <img width=\"1024\", src=\"https://user-images.githubusercontent.com/65292018/212471948-0bcf05da-5f10-45ae-a54a-38cf312c0909.jpg\"></a>\n","\n","\n","<br>\n","  <a href=\"https://colab.research.google.com/github/leemjm92/STEAMxD/blob/main/training-custom-model.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n","<br>\n","\n","This <a href=\"https://github.com/ultralytics/yolov5\">YOLOv5</a> üöÄ notebook by ROAR Lab presents simple train, validate and predict examples to help start your AI adventure.<br>Credit of this notebook goes to <a href=\"https://ultralytics.com/\">Ultralytics</a> and <a href=\"https://roboflow.com/\">Roboflow</a> as this notebook used both sample as reference.\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"7mGmQbAO5pQb"},"source":["# First time setup\n","\n","Use this section to setup your GDrive for YOLOv5 usage with google colab for the first time. **Once this initial setup is done for further usage you will not be required to download the YOLOv5 repository again. Please use the next section [here](#future-setup) when accessing this colab notebook for future usage.**\n","\n","In this section we're mounting GDrive, cloning GitHub [repository](https://github.com/ultralytics/yolov5), installing [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt), importing necessary libraries and checking PyTorch and GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"inGiY9WMawuu","scrolled":true},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# this creates a symbolic link so that now the path /content/gdrive/My Drive/ is equal to /mydrive\n","!ln -s /content/gdrive/My\\ Drive/ /mydrive\n","%cd /mydrive\n","!mkdir -p steamxd\n","%cd steamxd\n","!mkdir -p datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"joJiXAf3ejvH"},"outputs":[],"source":["%cd steamxd/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lCogvXNQawuv","scrolled":true},"outputs":[],"source":["!git clone https://github.com/ultralytics/yolov5  # clone\n","%cd yolov5\n","%pip install -qr requirements.txt  # install\n","\n","import torch\n","import utils\n","from IPython.display import Image, clear_output  # to display images\n","display = utils.notebook_init()  # checks"]},{"cell_type":"markdown","metadata":{"id":"ftcLJthqawuw"},"source":["# Future usage setup <a id=\"future-setup\"></a>\n","\n","If you already have YOLOv5 setup in your GDrive use this section to setup your colab notebook.\n","\n","In this section we're mounting GDrive, installing [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt), importing necessary libraries and checking PyTorch and GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHozpBBzawux","scrolled":true},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# this creates a symbolic link so that now the path /content/gdrive/My Drive/ is equal to /mydrive\n","!ln -s /content/gdrive/My\\ Drive/ /mydrive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wbvMlHd_QwMG","scrolled":true},"outputs":[],"source":["%cd /mydrive/steamxd/yolov5\n","%pip install -qr requirements.txt  # install\n","\n","import torch\n","import utils\n","from IPython.display import Image, clear_output  # to display images\n","display = utils.notebook_init()  # checks"]},{"cell_type":"markdown","metadata":{"id":"zY75YiLfawu8"},"source":["# 1. Training custom model\n","\n","Using the augmentation parameters that you've experimented in the [previous section](#image-aug), we will be training the custom model using your custom dataset.\n","\n","<!---\n","To edit the instructions to include Augmentation discussions for ease of understanding\n","\n","\n","`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n","\n","```shell\n","python detect.py --source 0  # webcam\n","                          img.jpg  # image \n","                          vid.mp4  # video\n","                          path/  # directory\n","                          'path/*.jpg'  # glob\n","                          'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n","                          'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n","```\n","-->"]},{"cell_type":"markdown","metadata":{"id":"aKCHJbQga8MU"},"source":["## Extract training images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CL0i-_xeRQNd"},"outputs":[],"source":["%cd /mydrive/steamxd/datasets/baseline/\n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gHh2_QE-t7ir"},"outputs":[],"source":["# %cd \"/mydrive/steamxd/datasets/sample/\"\n","!unzip ./baseline-dataset.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PexhtG7jfArf"},"outputs":[],"source":["%ls"]},{"cell_type":"markdown","metadata":{"id":"nevt5V0pbf7U"},"source":["## Define the image parameters for training"]},{"cell_type":"markdown","metadata":{"id":"pT_D8USKcbq-"},"source":["### Define image augmentation parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g1f7GLpnawu9","scrolled":true},"outputs":[],"source":["# how to rewrite the hyps yaml\n","%%writetemplate /mydrive/steamxd/yolov5/data/hyps/hyp.scratch-low.yaml\n","\n","# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license\n","# Hyperparameters for low-augmentation COCO training from scratch\n","# python train.py --batch 64 --cfg yolov5n6.yaml --weights '' --data coco.yaml --img 640 --epochs 300 --linear\n","# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n","\n","#################################\n","### DO NOT ALTER THIS PORTION ###\n","#################################\n","lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\n","lrf: 0.01  # final OneCycleLR learning rate (lr0 * lrf)\n","momentum: 0.937  # SGD momentum/Adam beta1\n","weight_decay: 0.0005  # optimizer weight decay 5e-4\n","warmup_epochs: 3.0  # warmup epochs (fractions ok)\n","warmup_momentum: 0.8  # warmup initial momentum\n","warmup_bias_lr: 0.1  # warmup initial bias lr\n","box: 0.05  # box loss gain\n","cls: 0.5  # cls loss gain\n","cls_pw: 1.0  # cls BCELoss positive_weight\n","obj: 1.0  # obj loss gain (scale with pixels)\n","obj_pw: 1.0  # obj BCELoss positive_weight\n","iou_t: 0.20  # IoU training threshold\n","anchor_t: 4.0  # anchor-multiple threshold\n","# anchors: 3  # anchors per output layer (0 to ignore)\n","fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n","#################################\n","### DO NOT ALTER THIS PORTION ###\n","#################################\n","###########################################\n","### Image Augmentation Parameters Start ###\n","###########################################\n","hsv_h: 0.015  # image HSV-Hue augmentation (fraction)\n","hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\n","hsv_v: 0.4  # image HSV-Value augmentation (fraction)\n","degrees: 30.0  # image rotation (+/- deg)\n","translate: 0.1  # image translation (+/- fraction)\n","scale: 0.0  # image scale (+/- gain)\n","shear: 0.0  # image shear (+/- deg)\n","perspective: 0.000  # image perspective (+/- fraction), range 0-0.001\n","flipud: 0.0  # image flip up-down (probability)\n","fliplr: 0.0  # image flip left-right (probability)\n","mosaic: 0.0  # image mosaic (probability)\n","mixup: 0.0  # image mixup (probability)\n","#########################################\n","### Image Augmentation Parameters End ###\n","#########################################\n","# copy_paste is for image segmentation leave this augmentation alone\n","copy_paste: 0.0  # segment copy-paste (probability)"]},{"cell_type":"markdown","metadata":{"id":"5pOSx2SqbwOi"},"source":["### Define the image path and the number of classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQfyGW0CbwOo"},"outputs":[],"source":["# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license\n","# COCO128 dataset https://www.kaggle.com/ultralytics/coco128 (first 128 images from COCO train2017) by Ultralytics\n","# Example usage: python train.py --data coco128.yaml\n","# parent\n","# ‚îú‚îÄ‚îÄ yolov5\n","# ‚îî‚îÄ‚îÄ datasets\n","#     ‚îî‚îÄ‚îÄ steamxd  ‚Üê downloads here (7 MB)\n","%%writetemplate /mydrive/steamxd/yolov5/data/baseline.yaml\n","\n","# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\n","path: /mydrive/steamxd/datasets/baseline  # dataset root dir\n","train: train/images  # train images (relative to 'path') 128 images\n","val: train/images  # val images (relative to 'path') 128 images\n","test:  # test images (optional)\n","\n","# Classes\n","names:\n","  0: humans\n","  1: pets"]},{"cell_type":"markdown","metadata":{"id":"GLyPofyrawu9"},"source":["### Define Model Configuration and Architecture\n","\n","We will write a yaml script that defines the parameters for our model like the number of classes, anchors, and each layer.\n","\n","The only line of code that you've to edit is `nc:` this is the parameter for the number of classes (e.g. `nc: 3` means you have 3 different classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5yg3qjBrbwOo"},"outputs":[],"source":["##### this is for medium model #####\n","%%writetemplate /mydrive/steamxd/yolov5/models/baseline.yaml\n","\n","# Parameters\n","nc: 2  # number of classes\n","depth_multiple: 0.67  # model depth multiple\n","width_multiple: 0.75  # layer channel multiple\n","anchors:\n","  - [10,13, 16,30, 33,23]  # P3/8\n","  - [30,61, 62,45, 59,119]  # P4/16\n","  - [116,90, 156,198, 373,326]  # P5/32\n","\n","# YOLOv5 v6.0 backbone\n","backbone:\n","  # [from, number, module, args]\n","  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n","   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n","   [-1, 3, C3, [128]],\n","   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n","   [-1, 6, C3, [256]],\n","   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n","   [-1, 9, C3, [512]],\n","   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n","   [-1, 3, C3, [1024]],\n","   [-1, 1, SPPF, [1024, 5]],  # 9\n","  ]\n","\n","# YOLOv5 v6.0 head\n","head:\n","  [[-1, 1, Conv, [512, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n","   [-1, 3, C3, [512, False]],  # 13\n","\n","   [-1, 1, Conv, [256, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n","   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n","\n","   [-1, 1, Conv, [256, 3, 2]],\n","   [[-1, 14], 1, Concat, [1]],  # cat head P4\n","   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n","\n","   [-1, 1, Conv, [512, 3, 2]],\n","   [[-1, 10], 1, Concat, [1]],  # cat head P5\n","   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n","\n","   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n","  ]"]},{"cell_type":"markdown","metadata":{"id":"wK8QgtF3bwOp"},"source":["## How to start training on your custom *model*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EJOEz4zobwOp"},"outputs":[],"source":["# ensure that you're in the parent yolov5 folder\n","%cd /mydrive/steamxd/yolov5"]},{"cell_type":"markdown","metadata":{"id":"_IF5LXpSbwOp"},"source":["`--img`: to specific the input image size 640x640 \\\n","`--batch`: the batch size of the image example input (16 images) [further explaination](https://developers.google.com/machine-learning/glossary#batch) \\\n","`--epochs`: A full training pass over the entire training set such that each example has been processed once. [further explaination](https://developers.google.com/machine-learning/glossary#epoch) \\\n","`--data`: configuration file for where the image dataset is stored \\\n","`--cfg`: configuration file for where your model architecture \\\n","`--weights`: pre-trained weights \\\n","`--save-period`: by default the library only saves the best and last weights to save weights after every 5 epoch type `--save-period 5`\\\n","`--cache`: store the training images in DRAM \\\n","`--name`: this is the folder name for different training runs (e.g. aug-mosaic, aug-mixup, etc) \\\n","`--project`: this is the directory path that you want to save your model runs in (e.g. '/mydrive/SUTD/STEAMxD') \\\n","`--resume`: this will resume training of your model if training is interrupted (e.g. epochs 100, training stops at epochs 55.) If all epochs has been done (100/100) but you still wish to continue training on the existing model use the `--weights` flag instead (e.g. --weights /mydrive/SUTD/STEAMxD/yolov5/runs/train/exp) \n","\n","<!---\n","--project can be removed if I'm gonna just be cloning the yolov5 repo into gdrive\n","--resume flag to resume training if you're initial training epochs (300 and your model stops as 150)\n","--weights alternatively you can use the --weights flag to specific the weights file to continue training from\n","--data I need to add in the yaml file edit so that I can link it to the correct dataset\n","I should add in and image to explain the epochs and batch size\n","-->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8aswfFlbwOp"},"outputs":[],"source":["# give some explanation of the different parameters\n","!python train.py --img 640 --batch 16 --epochs 200 --data ./data/baseline.yaml --cfg ./models/baseline.yaml --weights yolov5m.pt --save-period 5 --cache --name baseline --project '/mydrive/steamxd/yolov5/runs/train'"]},{"cell_type":"markdown","metadata":{"id":"sg-IxbEbHncd"},"source":["### How to see metrics and evaluate your model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOIc97dYbwOp"},"outputs":[],"source":["# Start tensorboard\n","# Launch after you have started training\n","# logs save in the folder \"runs\"\n","# if you see a google 403 error please ensure that your browser allows 3rd party cookie for tensorboard to be viewable\n","%load_ext tensorboard\n","%tensorboard --logdir /mydrive/steamxd/yolov5/runs/train/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9yu62iRvFQKv"},"outputs":[],"source":["%cd /mydrive/steamxd/yolov5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qz_U6JbbEJoi"},"outputs":[],"source":["!python detect.py --weights './runs/train/baseline/weights/best.pt' --source '/mydrive/steamxd/datasets/sample/frame1.jpg'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIdDO-DYFlmU"},"outputs":[],"source":["# print out test image\n","print(\"TEST IMAGE:\")\n","Image(filename='/mydrive/steamxd/yolov5/runs/detect/exp/frame1.jpg', width=900)"]},{"cell_type":"markdown","metadata":{"id":"rwqqmfvA_MtH"},"source":["### How to export your trained model weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0AfIMpWT2H8"},"outputs":[],"source":["#export your model's weights for future use\n","from google.colab import files\n","files.download('./runs/train/exp/weights/best.pt')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["7mGmQbAO5pQb","ftcLJthqawuw","RdePlB7GbAHY","1QIYtv8PQwzp","aKCHJbQga8MU","pT_D8USKcbq-","5pOSx2SqbwOi","GLyPofyrawu9"],"machine_shape":"hm","provenance":[{"file_id":"1wG77y_WXlRBl29FSoVg_YtpHCnqgPXY0","timestamp":1673700510841},{"file_id":"https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb","timestamp":1662297741718}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
