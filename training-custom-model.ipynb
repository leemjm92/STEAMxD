{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"t6MPjfT5NrKQ"},"source":["<div align=\"center\">\n","\n","  <a href=\"https://github.com/leemjm92/STEAMxD\" target=\"_blank\">\n","    <img width=\"1024\", src=\"https://user-images.githubusercontent.com/65292018/212471948-0bcf05da-5f10-45ae-a54a-38cf312c0909.jpg\"></a>\n","\n","\n","<br>\n","  <a href=\"https://colab.research.google.com/github/leemjm92/STEAMxD/blob/main/training-custom-model.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n","<br>\n","\n","This <a href=\"https://github.com/ultralytics/yolov5\">YOLOv5</a> üöÄ notebook by ROAR Lab presents simple train, validate and predict examples to help start your AI adventure.<br>Credit of this notebook goes to <a href=\"https://ultralytics.com/\">Ultralytics</a> and <a href=\"https://roboflow.com/\">Roboflow</a> as this notebook used both sample as reference.\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"7mGmQbAO5pQb"},"source":["# First time setup\n","\n","Use this section to setup your GDrive for YOLOv5 usage with google colab for the first time. **Once this initial setup is done for further usage you will not be required to download the YOLOv5 repository again. Please use the next section [here](#future-setup) when accessing this colab notebook for future usage.**\n","\n","In this section we're mounting GDrive, cloning GitHub [repository](https://github.com/ultralytics/yolov5), installing [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt), importing necessary libraries and checking PyTorch and GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"inGiY9WMawuu","scrolled":true},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# this creates a symbolic link so that now the path /content/gdrive/My Drive/ is equal to /mydrive\n","!ln -s /content/gdrive/My\\ Drive/ /mydrive\n","%cd /mydrive\n","!mkdir -p steamxd\n","%cd steamxd\n","!mkdir -p datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lCogvXNQawuv","scrolled":true},"outputs":[],"source":["!git clone https://github.com/ultralytics/yolov5  # clone\n","%cd yolov5\n","%pip install -qr requirements.txt  # install\n","\n","import torch\n","import utils\n","from IPython.display import Image, clear_output  # to display images\n","display = utils.notebook_init()  # checks"]},{"cell_type":"markdown","metadata":{"id":"ftcLJthqawuw"},"source":["# Future usage setup <a id=\"future-setup\"></a>\n","\n","If you already have YOLOv5 setup in your GDrive use this section to setup your colab notebook.\n","\n","In this section we're mounting GDrive, installing [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt), importing necessary libraries and checking PyTorch and GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHozpBBzawux","scrolled":true},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# this creates a symbolic link so that now the path /content/gdrive/My Drive/ is equal to /mydrive\n","!ln -s /content/gdrive/My\\ Drive/ /mydrive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wbvMlHd_QwMG","scrolled":true},"outputs":[],"source":["%cd /mydrive/steamxd/yolov5\n","%pip install -qr requirements.txt  # install\n","\n","import torch\n","import utils\n","from IPython.display import Image, clear_output  # to display images\n","display = utils.notebook_init()  # checks"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Download Link for baseline-dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["[Dataset can be obtained here](https://drive.google.com/file/d/1HdGEMcVGRpPXy4vDDokrgTVmsd80xePo/view?usp=sharing)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<img src=\"https://user-images.githubusercontent.com/65292018/212694986-1df02632-ec42-40c0-b187-3d3015b125ec.PNG\" alt=\"dataset-folders\" width=\"1000\"/>\n","<p style=\"text-align:left\">Once you've downloaded the dataset extract the zip file into a folder select all the 4 files extracted as seen in the picture below and upload it into the roboflow project space that has all your team members in</p>\n","<img src=\"https://user-images.githubusercontent.com/65292018/212694978-ec08ed33-7444-41bd-a334-1cb0e4874b6d.PNG\" alt=\"dataset-folders\" width=\"1000\"/>\n","<img src=\"https://user-images.githubusercontent.com/65292018/212695449-f600ad85-6b9b-4d39-8c87-ae3d4ce14bf6.png\" alt=\"dataset-folders\" width=\"600\"/>\n","<p style=\"text-align:left\">Once the images has been uploaded you should be see 898 total images, 849 of them are annotated and 49 of it are not annotated. You'll be required to annotate the other 49 images. Apart from that you will need to take additional images.</p>\n","<p style=\"text-align:left\"><b>Hint1: take more images in variety of angles you can approach the student helpers for pointers</b></p>\n","<p style=\"text-align:left\"><b>Hint2: The dataset is slightly imbalanced and has more instance of humans</b></p>\n","<img src=\"https://user-images.githubusercontent.com/65292018/212694970-1d123613-8a4b-446c-ac9c-256454bf43d5.PNG\" alt=\"dataset-folders\" width=\"1000\"/>\n","<p style=\"text-align:left\">The dataset will have 784 train images and 114 valid images. <b>Any additional images you've taken should be added into the train dataset</b></p>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## How to add more images to dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b>Instead of taking images individually, what you should do is place the objects on the table and take a video and then use roboflow to slice the video into images.</b> \\\n","<b>**NOTE: video must be in these 3 format .mov, .mp4 or .avi</b> \\\n","<img src=\"https://user-images.githubusercontent.com/65292018/212698178-7a457f60-b252-4484-a28a-2ec91edd5b3e.PNG\" alt=\"dataset-folders\" width=\"1000\"/>\n","<p style=\"text-align:left\">Drag and drop the video file into roboflow.</p>\n","<img src=\"https://user-images.githubusercontent.com/65292018/212698171-4da3f1e2-c9b0-47ac-a46b-c255d173291d.PNG\" alt=\"dataset-folders\" width=\"1000\"/>\n","<p style=\"text-align:left\">Once the video is uploaded you'll be prompted with the pop-up depending on how fast/slow you've panned your video you can toggle the frame/second requirements.</p>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["[![Video Sample](https://user-images.githubusercontent.com/65292018/212700158-aee5c610-dbbc-4436-bf58-4d2e3779ff92.PNG)](https://user-images.githubusercontent.com/65292018/212699268-d2f24297-c189-4bac-aab1-b184947f816e.mp4 \"Video Sample\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Setup the necessary variables (students only have to edit this segment of the code)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The pictures below are the folders/paths that will be of interest for the notebook. There is only 4 important folders to take note of 1. datasets, 2. yolov5/runs, 3. yolov5/runs/train & yolov5/runs/detect"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<img src=\"https://user-images.githubusercontent.com/65292018/212678190-f125452d-347d-41ed-bdeb-93fd5349947a.PNG\" alt=\"dataset-folders\" width=\"800\" height=\"600\"/> \n","<img src=\"https://user-images.githubusercontent.com/65292018/212680507-c8e3a047-183c-4764-8248-02539dd8a022.PNG\" alt=\"dataset-folders\" width=\"800\" height=\"600\"/> \n","<p style=\"text-align:left\">This is how the datasets folder should look like. <b>Always ensure that you place your exported labelled images from roboflow into the same folder name (e.g. abcd1234.zip, create a folder name in datasets named abcd1234 and then place the zip file in that folder)</b> an example of that image can be seen beside</p>\n","<img src=\"https://user-images.githubusercontent.com/65292018/212678186-4f5ce087-3591-4e17-86bd-c9e5a528f9be.PNG\" alt=\"detect-train-folder\" width=\"800\"> \n","<p style=\"text-align:left\">In the yolov5/runs folder you will see two folder <b>once you've at least execute !python train.py ... or !python detect.py ... once</b></p>\n","<img src=\"https://user-images.githubusercontent.com/65292018/212678183-195b00d1-6a7a-4cd4-8dcc-8427127528b5.PNG\" alt=\"detect-train-folder2\" width=\"800\"/> \n","<p style=\"text-align:left\">Within the yolov5/runs/ detect or train folder you can see that they are name slightly different. By default the naming convention for the folders will be exp, the folders will increase in integer (e.g. exp1, exp2, exp3, exp4, etc). <b>To change the name to the necessary folder be look at the description above the !python train.py or !python detect.py code and look at the --name component</b></p>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Dataset File Name ###\n","dataset_filename = \"baseline-dataset\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Model Configurations"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Image Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hsv_h: 0.015  # image HSV-Hue augmentation (fraction)\n","hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\n","hsv_v: 0.4  # image HSV-Value augmentation (fraction)\n","degrees: 20.0  # image rotation (+/- deg)\n","translate: 0.1  # image translation (+/- fraction)\n","scale: 0.9  # image scale (+/- gain)\n","shear: 0.0  # image shear (+/- deg)\n","perspective: 0.0  # image perspective (+/- fraction), range 0-0.001\n","flipud: 0.0  # image flip up-down (probability)\n","fliplr: 0.0  # image flip left-right (probability)\n","mosaic: 0.0  # image mosaic (probability)\n","mixup: 0.0  # image mixup (probability)"]},{"cell_type":"markdown","metadata":{"id":"zY75YiLfawu8"},"source":["# 1. Training custom model\n","\n","Using the augmentation parameters that you've experimented in the [previous section](#image-aug), we will be training the custom model using your custom dataset.\n","\n","<!---\n","To edit the instructions to include Augmentation discussions for ease of understanding\n","\n","\n","`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n","\n","```shell\n","python detect.py --source 0  # webcam\n","                          img.jpg  # image \n","                          vid.mp4  # video\n","                          path/  # directory\n","                          'path/*.jpg'  # glob\n","                          'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n","                          'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n","```\n","-->"]},{"cell_type":"markdown","metadata":{"id":"aKCHJbQga8MU"},"source":["## Extract training images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CL0i-_xeRQNd"},"outputs":[],"source":["%cd /mydrive/steamxd/datasets/{dataset_filename}/\n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gHh2_QE-t7ir"},"outputs":[],"source":["!unzip ./{dataset_filename}.zip\n","%ls"]},{"cell_type":"markdown","metadata":{"id":"nevt5V0pbf7U"},"source":["## Define the image parameters for training"]},{"cell_type":"markdown","metadata":{"id":"pT_D8USKcbq-"},"source":["### Define image augmentation parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g1f7GLpnawu9","scrolled":true},"outputs":[],"source":["# how to rewrite the hyps yaml\n","%%writetemplate /mydrive/steamxd/yolov5/data/hyps/hyp.scratch-low.yaml\n","\n","# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license\n","# Hyperparameters for low-augmentation COCO training from scratch\n","# python train.py --batch 64 --cfg yolov5n6.yaml --weights '' --data coco.yaml --img 640 --epochs 300 --linear\n","# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n","\n","#################################\n","### DO NOT ALTER THIS PORTION ###\n","#################################\n","lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\n","lrf: 0.01  # final OneCycleLR learning rate (lr0 * lrf)\n","momentum: 0.937  # SGD momentum/Adam beta1\n","weight_decay: 0.0005  # optimizer weight decay 5e-4\n","warmup_epochs: 3.0  # warmup epochs (fractions ok)\n","warmup_momentum: 0.8  # warmup initial momentum\n","warmup_bias_lr: 0.1  # warmup initial bias lr\n","box: 0.05  # box loss gain\n","cls: 0.5  # cls loss gain\n","cls_pw: 1.0  # cls BCELoss positive_weight\n","obj: 1.0  # obj loss gain (scale with pixels)\n","obj_pw: 1.0  # obj BCELoss positive_weight\n","iou_t: 0.20  # IoU training threshold\n","anchor_t: 4.0  # anchor-multiple threshold\n","# anchors: 3  # anchors per output layer (0 to ignore)\n","fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n","#################################\n","### DO NOT ALTER THIS PORTION ###\n","#################################\n","###########################################\n","### Image Augmentation Parameters Start ###\n","###########################################\n","hsv_h: {hsv_h}  # image HSV-Hue augmentation (fraction)\n","hsv_s: {hsv_s}  # image HSV-Saturation augmentation (fraction)\n","hsv_v: {hsv_v}  # image HSV-Value augmentation (fraction)\n","degrees: {degrees}  # image rotation (+/- deg)\n","translate: {translate}  # image translation (+/- fraction)\n","scale: {scale}  # image scale (+/- gain)\n","shear: {shear} # image shear (+/- deg)\n","perspective: {perspective}  # image perspective (+/- fraction), range 0-0.001\n","flipud: {flipud}  # image flip up-down (probability)\n","fliplr: {fliplr}  # image flip left-right (probability)\n","mosaic: {mosaic}  # image mosaic (probability)\n","mixup: {mixup}  # image mixup (probability)\n","#########################################\n","### Image Augmentation Parameters End ###\n","#########################################\n","# copy_paste is for image segmentation leave this augmentation alone\n","copy_paste: 0.0  # segment copy-paste (probability)"]},{"cell_type":"markdown","metadata":{"id":"5pOSx2SqbwOi"},"source":["### Define the image path and the number of classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQfyGW0CbwOo"},"outputs":[],"source":["# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license\n","# COCO128 dataset https://www.kaggle.com/ultralytics/coco128 (first 128 images from COCO train2017) by Ultralytics\n","# Example usage: python train.py --data coco128.yaml\n","# parent\n","# ‚îú‚îÄ‚îÄ yolov5\n","# ‚îî‚îÄ‚îÄ datasets\n","#     ‚îî‚îÄ‚îÄ steamxd  ‚Üê downloads here (7 MB)\n","%%writetemplate /mydrive/steamxd/yolov5/data/{dataset_filename}.yaml\n","\n","# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\n","path: /mydrive/steamxd/datasets/{dataset_filename}  # dataset root dir\n","train: train/images  # train images (relative to 'path') 128 images\n","val: val/images  # val images (relative to 'path') 128 images\n","test:  # test images (optional)\n","\n","# Classes\n","names:\n","  0: humans\n","  1: pets"]},{"cell_type":"markdown","metadata":{"id":"GLyPofyrawu9"},"source":["### Define Model Configuration and Architecture\n","\n","We will write a yaml script that defines the parameters for our model like the number of classes, anchors, and each layer.\n","\n","The only line of code that you've to edit is `nc:` this is the parameter for the number of classes (e.g. `nc: 3` means you have 3 different classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5yg3qjBrbwOo"},"outputs":[],"source":["##### this is for medium model #####\n","%%writetemplate /mydrive/steamxd/yolov5/models/{dataset_filename}.yaml\n","\n","# Parameters\n","nc: 2  # number of classes\n","depth_multiple: 0.67  # model depth multiple\n","width_multiple: 0.75  # layer channel multiple\n","anchors:\n","  - [10,13, 16,30, 33,23]  # P3/8\n","  - [30,61, 62,45, 59,119]  # P4/16\n","  - [116,90, 156,198, 373,326]  # P5/32\n","\n","# YOLOv5 v6.0 backbone\n","backbone:\n","  # [from, number, module, args]\n","  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n","   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n","   [-1, 3, C3, [128]],\n","   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n","   [-1, 6, C3, [256]],\n","   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n","   [-1, 9, C3, [512]],\n","   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n","   [-1, 3, C3, [1024]],\n","   [-1, 1, SPPF, [1024, 5]],  # 9\n","  ]\n","\n","# YOLOv5 v6.0 head\n","head:\n","  [[-1, 1, Conv, [512, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n","   [-1, 3, C3, [512, False]],  # 13\n","\n","   [-1, 1, Conv, [256, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n","   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n","\n","   [-1, 1, Conv, [256, 3, 2]],\n","   [[-1, 14], 1, Concat, [1]],  # cat head P4\n","   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n","\n","   [-1, 1, Conv, [512, 3, 2]],\n","   [[-1, 10], 1, Concat, [1]],  # cat head P5\n","   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n","\n","   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n","  ]"]},{"cell_type":"markdown","metadata":{"id":"wK8QgtF3bwOp"},"source":["## How to start training on your custom *model*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EJOEz4zobwOp"},"outputs":[],"source":["# ensure that you're in the parent yolov5 folder\n","%cd /mydrive/steamxd/yolov5"]},{"cell_type":"markdown","metadata":{"id":"_IF5LXpSbwOp"},"source":["`--img`: to specific the input image size 640x640 \\\n","`--batch`: the batch size of the image example input (16 images) [further explaination](https://developers.google.com/machine-learning/glossary#batch) \\\n","`--epochs`: A full training pass over the entire training set such that each example has been processed once. [further explaination](https://developers.google.com/machine-learning/glossary#epoch) \\\n","`--data`: configuration file for where the image dataset is stored \\\n","`--cfg`: configuration file for where your model architecture \\\n","`--weights`: pre-trained weights \\\n","`--save-period`: by default the library only saves the best and last weights to save weights after every 5 epoch type `--save-period 5`\\\n","`--cache`: store the training images in DRAM \\\n","`--name`: this is the folder name for different training runs (e.g. aug-mosaic, aug-mixup, etc) \\\n","`--project`: this is the directory path that you want to save your model runs in (e.g. '/mydrive/SUTD/STEAMxD') \\\n","`--resume`: this will resume training of your model if training is interrupted (e.g. epochs 100, training stops at epochs 55.) If all epochs has been done (100/100) but you still wish to continue training on the existing model use the `--weights` flag instead (e.g. --weights /mydrive/SUTD/STEAMxD/yolov5/runs/train/exp) \n","\n","<!---\n","--project can be removed if I'm gonna just be cloning the yolov5 repo into gdrive\n","--resume flag to resume training if you're initial training epochs (300 and your model stops as 150)\n","--weights alternatively you can use the --weights flag to specific the weights file to continue training from\n","--data I need to add in the yaml file edit so that I can link it to the correct dataset\n","I should add in and image to explain the epochs and batch size\n","-->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8aswfFlbwOp"},"outputs":[],"source":["# give some explanation of the different parameters\n","!python train.py --img 640 --batch 16 --epochs 200 --data ./data/{dataset_filename}.yaml --cfg ./models/{dataset_filename}.yaml --weights yolov5m.pt --save-period 5 --cache --name {dataset_filename} --project '/mydrive/steamxd/yolov5/runs/train'"]},{"cell_type":"markdown","metadata":{"id":"sg-IxbEbHncd"},"source":["### How to see metrics and evaluate your model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["mAP 0.50, mAP 0.50~0.95, precision & recall: to evaluate the different models the higher the value is the \"better\" the model is \\\n","box_loss, cls_loss, obj_loss: the lower the \"better\" before the graphs starts to plateau"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOIc97dYbwOp"},"outputs":[],"source":["# Start tensorboard\n","# Launch after you have started training\n","# logs save in the folder \"runs\"\n","# if you see a google 403 error please ensure that your browser allows 3rd party cookie for tensorboard to be viewable\n","%load_ext tensorboard\n","%tensorboard --logdir /mydrive/steamxd/yolov5/runs/train/"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## How to do inference on your trained model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<img src=\"https://user-images.githubusercontent.com/65292018/212684527-045627d5-177c-482a-a04d-fb5ebcc56aad.PNG\" alt=\"inference-folder\" width=\"45%\"/>\n","<img src=\"https://user-images.githubusercontent.com/65292018/212685246-1364818a-a479-4e43-8bb2-e7d8a5e1c9a4.PNG\" alt=\"inference-folder\" width=\"45%\"/> \n","<p style=\"text-align:left\">Here you can create an inference folder and place <b>NEW images</b> to test if your model is performing well on new data</p>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Setup for the necessary variables for inference (students please edit this portion)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### inference folder name ###\n","inference_folder = \"inference\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9yu62iRvFQKv"},"outputs":[],"source":["%cd /mydrive/steamxd/yolov5"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As mentioned you can use \\\n","```--name```: this is the folder name for different training runs (e.g. inference-set1, inference-pets1, etc) \\\n","```--source```: the image folder source can be used (e.g. /mydrive/steamxd/datasets/inference) all the images in this folder will be infered"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qz_U6JbbEJoi"},"outputs":[],"source":["!python detect.py --weights ./runs/train/{inference_folder}/weights/best.pt --source /mydrive/steamxd/datasets/{inference_folder}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["If you're using the default naming scheme (e.g. exp, exp1, exp2, etc). Do remember to change the filepath below after ```.../yolov5/runs/detect/```[please edit this portion]```/```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIdDO-DYFlmU"},"outputs":[],"source":["# print out test image\n","print(\"TEST IMAGE:\")\n","Image(filename='/mydrive/steamxd/yolov5/runs/detect/exp/frame1.jpg', width=900)"]},{"cell_type":"markdown","metadata":{"id":"rwqqmfvA_MtH"},"source":["### How to export your trained model weights"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Similarly do change the paths that you're export the weights from ```.../yolov5/runs/train/```[please edit this portion]```/weights/best.pt```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0AfIMpWT2H8"},"outputs":[],"source":["#export your model's weights for future use\n","from google.colab import files\n","files.download('./runs/train/exp/weights/best.pt')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["7mGmQbAO5pQb","ftcLJthqawuw","RdePlB7GbAHY","1QIYtv8PQwzp","aKCHJbQga8MU","pT_D8USKcbq-","5pOSx2SqbwOi","GLyPofyrawu9"],"machine_shape":"hm","provenance":[{"file_id":"1wG77y_WXlRBl29FSoVg_YtpHCnqgPXY0","timestamp":1673700510841},{"file_id":"https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb","timestamp":1662297741718}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1 (tags/v3.8.1:1b293b6, Dec 18 2019, 23:11:46) [MSC v.1916 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"fd9a3f95d60962a936c2f57e8e13ba0f0dd2b320c32841c95743504c68ce9fb6"}}},"nbformat":4,"nbformat_minor":0}
