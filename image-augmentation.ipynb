{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"t6MPjfT5NrKQ"},"source":["<div align=\"center\">\n","\n","  <a href=\"https://github.com/leemjm92/STEAMxD\" target=\"_blank\">\n","    <img width=\"1024\", src=\"https://user-images.githubusercontent.com/65292018/212471948-0bcf05da-5f10-45ae-a54a-38cf312c0909.jpg\"></a>\n","\n","\n","<br>\n","  <a href=\"https://colab.research.google.com/github/leemjm92/STEAMxD/blob/main/image-augmentation.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n","<br>\n","\n","This <a href=\"https://github.com/ultralytics/yolov5\">YOLOv5</a> 🚀 notebook by ROAR Lab presents simple train, validate and predict examples to help start your AI adventure.<br>Credit of this notebook goes to <a href=\"https://ultralytics.com/\">Ultralytics</a> and <a href=\"https://roboflow.com/\">Roboflow</a> as this notebook used both sample as reference.\n","\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"7mGmQbAO5pQb"},"source":["# First time setup\n","\n","Use this section to setup your GDrive for YOLOv5 usage with google colab for the first time. **Once this initial setup is done for further usage you will not be required to download the YOLOv5 repository again. Please use the next section [here](#future-setup) when accessing this colab notebook for future usage.**\n","\n","In this section we're mounting GDrive, cloning GitHub [repository](https://github.com/ultralytics/yolov5), installing [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt), importing necessary libraries and checking PyTorch and GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"inGiY9WMawuu","scrolled":true},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# this creates a symbolic link so that now the path /content/gdrive/My Drive/ is equal to /mydrive\n","!ln -s /content/gdrive/My\\ Drive/ /mydrive\n","%cd /mydrive\n","!mkdir -p steamxd\n","%cd steamxd\n","!mkdir -p datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lCogvXNQawuv","scrolled":true},"outputs":[],"source":["!git clone https://github.com/ultralytics/yolov5  # clone\n","%cd yolov5\n","%pip install -qr requirements.txt  # install\n","\n","import torch\n","import utils\n","from IPython.display import Image, clear_output  # to display images\n","display = utils.notebook_init()  # checks"]},{"cell_type":"markdown","metadata":{"id":"ftcLJthqawuw"},"source":["# Future usage setup <a id=\"future-setup\"></a>\n","\n","If you already have YOLOv5 setup in your GDrive use this section to setup your colab notebook.\n","\n","In this section we're mounting GDrive, installing [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt), importing necessary libraries and checking PyTorch and GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHozpBBzawux","scrolled":true},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# this creates a symbolic link so that now the path /content/gdrive/My Drive/ is equal to /mydrive\n","!ln -s /content/gdrive/My\\ Drive/ /mydrive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wbvMlHd_QwMG","scrolled":true},"outputs":[],"source":["%cd /mydrive/steamxd/yolov5\n","%pip install -qr requirements.txt  # install\n","\n","import torch\n","import utils\n","from IPython.display import Image, clear_output  # to display images\n","display = utils.notebook_init()  # checks"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Setup the necessary variables (students only have to edit this segment of the code)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The pictures below are the folders/paths that will be of interest for the notebook. There is only 4 important folders to take note of 1. datasets, 2. yolov5/runs, 3. yolov5/runs/train & yolov5/runs/detect"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<img src=\"https://user-images.githubusercontent.com/65292018/212678190-f125452d-347d-41ed-bdeb-93fd5349947a.PNG\" alt=\"dataset-folders\" width=\"800\" height=\"600\"/> \n","<img src=\"https://user-images.githubusercontent.com/65292018/212680507-c8e3a047-183c-4764-8248-02539dd8a022.PNG\" alt=\"dataset-folders\" width=\"800\" height=\"600\"/> \n","<p style=\"text-align:left\">This is how the datasets folder should look like. <b>Always ensure that you place your exported labelled images from roboflow into the same folder name (e.g. abcd1234.zip, create a folder name in datasets named abcd1234 and then place the zip file in that folder)</b> an example of that image can be seen beside</p>\n","<img src=\"https://user-images.githubusercontent.com/65292018/212678186-4f5ce087-3591-4e17-86bd-c9e5a528f9be.PNG\" alt=\"detect-train-folder\" width=\"800\"> \n","<p style=\"text-align:left\">In the yolov5/runs folder you will see two folder <b>once you've at least execute !python train.py ... or !python detect.py ... once</b></p>\n","<img src=\"https://user-images.githubusercontent.com/65292018/212678183-195b00d1-6a7a-4cd4-8dcc-8427127528b5.PNG\" alt=\"detect-train-folder2\" width=\"800\"/> \n","<p style=\"text-align:left\">Within the yolov5/runs/ detect or train folder you can see that they are name slightly different. By default the naming convention for the folders will be exp, the folders will increase in integer (e.g. exp1, exp2, exp3, exp4, etc). <b>To change the name to the necessary folder be look at the description above the !python train.py or !python detect.py code and look at the --name component</b></p>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Dataset File Name ###\n","dataset_filename = \"sample\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Model Configurations"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Image Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hsv_h =  0.0  # image HSV-Hue augmentation (fraction)\n","hsv_s = 0.0  # image HSV-Saturation augmentation (fraction)\n","hsv_v = 0.0  # image HSV-Value augmentation (fraction)\n","degrees = 0.0  # image rotation (+/- deg)\n","translate = 0.0  # image translation (+/- fraction)\n","scale = 0.0  # image scale (+/- gain)\n","shear = 0.0  # image shear (+/- deg)\n","perspective = 0.000  # image perspective (+/- fraction), range 0-0.001\n","flipud = 0.0  # image flip up-down (probability)\n","fliplr = 0.0  # image flip left-right (probability)\n","mosaic = 0.5  # image mosaic (probability)\n","mixup = 0.0  # image mixup (probability)"]},{"cell_type":"markdown","metadata":{"id":"4JnkELT0cIJg"},"source":["# 1. Image Augmentation <a id=\"image-aug\"></a>\n","\n","In this example we will do a single augmentation (mosaic) and train the model for a single epoch to showcase the available augmentations while training.\n","\n","<!---\n","To edit the instructions to include Augmentation discussions for ease of understanding\n","\n","\n","`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n","\n","```shell\n","python detect.py --source 0  # webcam\n","                          img.jpg  # image \n","                          vid.mp4  # video\n","                          path/  # directory\n","                          'path/*.jpg'  # glob\n","                          'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n","                          'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n","```\n","-->"]},{"cell_type":"markdown","metadata":{"id":"RdePlB7GbAHY"},"source":["## Interactive Augmentation Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9RANzHc4awu0","scrolled":true},"outputs":[],"source":["import ipywidgets as widgets\n","import numpy as np\n","import subprocess\n","import platform\n","import math\n","import random\n","import cv2\n","import os\n","import urllib.request\n","import matplotlib.pyplot as plt\n","\n","#customize iPython writefile so we can write variables\n","from IPython.core.magic import register_line_cell_magic\n","\n","@register_line_cell_magic\n","def writetemplate(line, cell):\n","    with open(line, 'w') as f:\n","        f.write(cell.format(**globals()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCEj-lFxbuTN"},"outputs":[],"source":["def load_image(filename):\n","    \"\"\"\n","    image path for linux and windows\n","    \"\"\"\n","\n","    # Option for the number of packets as a function of\n","    separator = '\\\\' if platform.system().lower()=='windows' else '/'\n","\n","    # Building the command. Ex: \"ping -c 1 google.com\"\n","    paths = [\".\", \"data\", \"images\", filename]\n","\n","    image_path = separator.join(paths)\n","\n","    return image_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gg0nz_MkfME3"},"outputs":[],"source":["'''\n","![cat1](https://user-images.githubusercontent.com/65292018/203496183-3eab4c82-ef65-4a74-8af8-2fff1edf7db0.jpg)\n","![human1](https://user-images.githubusercontent.com/65292018/203496190-3dc5e358-f882-42b9-bfc1-8f4c5cd048c1.jpg)\n","![lights1](https://user-images.githubusercontent.com/65292018/203496192-a2173786-e8cd-4818-b116-fcc87701f036.jpg)\n","![phone1](https://user-images.githubusercontent.com/65292018/203496194-d4eac98f-84ef-4e89-9df5-ba1a7915d937.jpg)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GQLn2nM4awu1","scrolled":true},"outputs":[],"source":["augmentation_list = [\"HSV\", \"Rotation\", \"Translate\", \"Scale\", \"Shear\", \"Flip\", \"Mosaic\", \"Mixup\"]\n","# remember to change to / for google colab I'm currently testing on windows\n","\n","# load from url instead\n","req = urllib.request.urlopen('https://user-images.githubusercontent.com/65292018/203496190-3dc5e358-f882-42b9-bfc1-8f4c5cd048c1.jpg')\n","arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n","img = cv2.imdecode(arr, -1) # 'Load it as it is'\n","\n","req2 = urllib.request.urlopen('https://user-images.githubusercontent.com/65292018/203496192-a2173786-e8cd-4818-b116-fcc87701f036.jpg')\n","arr2 = np.asarray(bytearray(req2.read()), dtype=np.uint8)\n","img2 = cv2.imdecode(arr2, -1)\n","\n","req3 = urllib.request.urlopen('https://user-images.githubusercontent.com/65292018/203496194-d4eac98f-84ef-4e89-9df5-ba1a7915d937.jpg')\n","arr3 = np.asarray(bytearray(req3.read()), dtype=np.uint8)\n","img3 = cv2.imdecode(arr3, -1)\n","\n","req4 = urllib.request.urlopen('https://user-images.githubusercontent.com/65292018/203496183-3eab4c82-ef65-4a74-8af8-2fff1edf7db0.jpg')\n","arr4 = np.asarray(bytearray(req4.read()), dtype=np.uint8)\n","img4 = cv2.imdecode(arr4, -1)\n","\n","# img = cv2.imread(load_image(\"human1.jpg\"))\n","# img2 = cv2.imread(load_image(\"lights1.jpg\"))\n","# img3 = cv2.imread(load_image(\"phone1.jpg\"))\n","# img4 = cv2.imread(load_image(\"cat1.jpg\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DST06npKawu3","scrolled":true},"outputs":[],"source":["##################################\n","### functions for augmentation ###\n","##################################\n","\n","def show_image(img):\n","    # convert from BGR to RGB as cv2 uses BGR color space and plt uses RGB\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    # scaling the image\n","    scale_percent = 200 # percent of original size\n","    width = int(img.shape[1] * scale_percent / 100)\n","    height = int(img.shape[0] * scale_percent / 100)\n","    dim = (width, height)  \n","    img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n","    \n","    plt.imshow(img);\n","\n","def hsv_image(hue, saturation, value):\n","    hue_ori, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))\n","    dtype = img.dtype\n","    x = np.arange(0, 256, dtype=dtype)\n","    lut_hue = ((x * hue) % 180).astype(dtype)\n","    lut_sat = np.clip(x * saturation, 0, 255).astype(dtype)\n","    lut_val = np.clip(x * value, 0, 255).astype(dtype)\n","\n","    img_hsv = cv2.merge((cv2.LUT(hue_ori, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val)))\n","    img_hsv = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR)\n","\n","    show_image(img_hsv)\n","\n","def rotation_image(rotation_rate):\n","    # PIL .size(width, height), cv2 .shape(height, width, channel)\n","    h, w, _ = img.shape\n","    M = cv2.getRotationMatrix2D((int(w/2), int(h/2)), rotation_rate, 1)\n","    img_rot = cv2.warpAffine(img, M, (w, h), borderValue=(114, 114, 114))\n","    show_image(img_rot)\n","    \n","def translate_image(translate_rate):\n","    h, w, _ = img.shape\n","    T = np.eye(3)\n","    T[0, 2] = translate_rate * w\n","    T[1, 2] = translate_rate * h\n","    img_translate = cv2.warpAffine(src=img, M=T[:2], dsize=(w, h), borderValue=(114, 114, 114))\n","    show_image(img_translate)\n","    \n","def scale_image(scale_rate):\n","    h, w, _ = img.shape\n","    center = (w/2, h/2)\n","    scale = 1 + scale_rate\n","    scale_matrix = cv2.getRotationMatrix2D(angle=0, center=center, scale=scale)\n","    img_scale = cv2.warpAffine(src=img, M=scale_matrix, dsize=(w, h), borderValue=(114, 114, 114))\n","    show_image(img_scale)\n","    \n","def shear_image(shear_rate):\n","    h, w, _ = img.shape\n","    S = np.eye(3)\n","    S[0, 1] = math.tan(shear_rate * math.pi / 180)  # x shear (deg)\n","    S[1, 0] = math.tan(shear_rate * math.pi / 180)  # y shear (deg)\n","    img_shear = cv2.warpAffine(src=img, M=S[:2], dsize=(w, h), borderValue=(114, 114, 114))\n","    show_image(img_shear)\n","    \n","def flip_image(flip):\n","    if flip == \"up/down\":\n","        flip_value = 0\n","    elif flip == \"left/right\":\n","        flip_value = 1\n","    img_flipud = cv2.flip(img, flip_value)\n","    show_image(img_flipud)\n","    \n","def random_rotation(img):\n","    angle = [45, 90, -45, -90, 0, 180]\n","    rotation_rate = random.choice(angle)\n","    h, w, _ = img.shape\n","    M = cv2.getRotationMatrix2D((int(w/2), int(h/2)), rotation_rate, 1)\n","    img_rot = cv2.warpAffine(img, M, (w, h), borderValue=(114, 114, 114))\n","    return img_rot\n","    \n","def mosaic_image(scale):\n","    scale_range = (scale, 1-scale)\n","    h, w, _ = img.shape\n","    img_mosaic = np.zeros([h, w, 3], dtype=np.uint8)\n","    scale_x = scale_range[0] + random.random() * (scale_range[1] - scale_range[0])\n","    scale_y = scale_range[0] + random.random() * (scale_range[1] - scale_range[0])\n","    divid_point_x = int(scale_x * w)\n","    divid_point_y = int(scale_y * h)\n","\n","    # top-left\n","    img_1 = cv2.resize(img, (divid_point_x, divid_point_y))\n","    img_1 = random_rotation(img_1)\n","    img_mosaic[:divid_point_y, :divid_point_x, :] = img_1\n","\n","    # top-right\n","    img_2 = cv2.resize(img2, (w - divid_point_x, divid_point_y))\n","    img_2 = random_rotation(img_2)\n","    img_mosaic[:divid_point_y, divid_point_x:w, :] = img_2\n","\n","    # btm-left\n","    img_3 = cv2.resize(img3, (divid_point_x, h - divid_point_y))\n","    img_3 = random_rotation(img_3)\n","    img_mosaic[divid_point_y:h, :divid_point_x, :] = img_3\n","\n","    # btm-right\n","    img_4 = cv2.resize(img4, (w - divid_point_x, h - divid_point_y))\n","    img_4 = random_rotation(img_4)\n","    img_mosaic[divid_point_y:h, divid_point_x:w, :] = img_4\n","\n","    show_image(img_mosaic)\n","    \n","def mixup_image(alpha):\n","    new_resize = (256, 256)\n","    img_1 = cv2.resize(img, new_resize, interpolation=cv2.INTER_AREA)\n","    img_2 = cv2.resize(img2, new_resize, interpolation=cv2.INTER_AREA)\n","    beta = (1.0 - alpha)\n","    img_mixup = cv2.addWeighted(img_1, alpha, img_2, beta, 0.0)\n","    show_image(img_mixup)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_NISo4Zawu3","scrolled":true},"outputs":[],"source":["########################################\n","### function for augmentation widget ###\n","########################################\n","\n","def augmentation_widget(augmentation):\n","    if augmentation == \"HSV\":\n","        hue_slider = widgets.FloatSlider(min=0, max=1, step=0.1, value=1)\n","        value_slider = widgets.FloatSlider(min=0, max=1, step=0.1, value=1)\n","        saturation_slider = widgets.FloatSlider(min=0, max=1, step=0.1, value=1)\n","        widgets.interact(hsv_image, hue=hue_slider, saturation=saturation_slider, value=value_slider);\n","        \n","    elif augmentation == \"Rotation\":\n","        rotation_slider = widgets.IntSlider(min=0, max=360, step=10, value=40)\n","        widgets.interact(rotation_image, rotation_rate=rotation_slider);\n","        \n","    elif augmentation == \"Translate\":\n","        translate_slider = widgets.FloatSlider(min=-1, max=1, step=0.1, value=0.2)\n","        widgets.interact(translate_image, translate_rate=translate_slider);\n","        \n","    elif augmentation == \"Scale\":\n","        scale_slider = widgets.FloatSlider(min=-1, max=1, step=0.1, value=-0.4)\n","        widgets.interact(scale_image, scale_rate=scale_slider);\n","    \n","    elif augmentation == \"Shear\":\n","        shear_slider = widgets.IntSlider(min=-20, max=20, step=1, value=5)\n","        widgets.interact(shear_image, shear_rate=shear_slider);\n","    \n","    elif augmentation == \"Flip\":\n","        flip_list = [\"up/down\", \"left/right\"]\n","        widgets.interact(flip_image, flip=flip_list);\n","             \n","    elif augmentation == \"Mosaic\":\n","        mosaic_slider = widgets.FloatSlider(min=0.2, max=0.9, step=0.1, value=0.3)\n","        widgets.interact(mosaic_image, scale=mosaic_slider);\n","    \n","    elif augmentation == \"Mixup\":\n","        mixup_slider = widgets.FloatSlider(min=-0.1, max=0.9, step=0.1, value=0.5)\n","        widgets.interact(mixup_image, alpha=mixup_slider);"]},{"cell_type":"markdown","metadata":{"id":"2R6kKnSrbJPN"},"source":["## Interactive Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGQx4ZeDawu3","scrolled":false},"outputs":[],"source":["widgets.interact(augmentation_widget, augmentation=augmentation_list);"]},{"cell_type":"markdown","metadata":{"id":"aMj7dElrawu4"},"source":["## Image Augmentation Parameters\n","\n","<!-- - To add some simple explanation on the augmentation done\n","\n","| | Image Augmentation Examples | |\n","| :-: | :-: | :-: |\n","| <div><img src=\"https://user-images.githubusercontent.com/65292018/195560245-3d7a7b5e-13b0-4880-8116-339d50128723.jpg\" width=\"250\"/></div> | <div><img src=\"https://user-images.githubusercontent.com/65292018/195560259-988483e1-7ae1-476b-85c4-fccc35dd6741.jpg\" width=\"250\"/></div> | <div><img src=\"https://user-images.githubusercontent.com/65292018/195560275-87b11837-1891-4446-b06b-9ee33bf57357.jpg\" width=\"250\"/></div> |\n","| HSV-Hue    | HSV-Saturation    | HSV-Value |\n","| <div><img src=\"https://user-images.githubusercontent.com/65292018/195560226-674913b9-3042-4910-bcce-976ea12393b9.jpg\" width=\"250\"/></div> | <div><img src=\"https://user-images.githubusercontent.com/65292018/195560269-562f7e81-e6d3-413b-89f8-3d3dcecaf3b5.jpg\" width=\"250\"/></div> | <div><img src=\"https://user-images.githubusercontent.com/65292018/195560262-7eae030e-0d10-4b81-ab2f-0c18e26303b5.jpg\" width=\"250\"/></div> |\n","| Rotation   | Translation       | Scale     |\n","| <div><img src=\"https://user-images.githubusercontent.com/65292018/195560265-3e6d1ecf-142b-4ce4-9024-30f1a0b31159.jpg\" width=\"250\"/></div> | <div><img src=\"https://user-images.githubusercontent.com/65292018/195560255-8737528e-6c9a-4292-9381-7776a43ae386.jpg\" width=\"250\"/></div> | <div><img src=\"https://user-images.githubusercontent.com/65292018/195560237-9003d9af-572e-496e-896e-b3a471a6c3cf.jpg\" width=\"250\"/></div> |\n","| Shear      | Perspective       | Flip (up/down) |\n","| <div><img src=\"https://user-images.githubusercontent.com/65292018/195560232-12feddfe-c06d-4e0e-ba7c-528a7731ee79.jpg\" width=\"250\"/></div> | <div><img src=\"https://user-images.githubusercontent.com/65292018/195560251-3a13aa0f-4b95-4a1f-9da8-f6cc69ec812c.jpg\" width=\"250\"/></div> | <div><img src=\"https://user-images.githubusercontent.com/65292018/195560250-c1656e46-9fc9-42f6-a97f-d5368b5650fb.jpg\" width=\"250\"/></div> |\n","| Flip (left/right)   | Mosaic   | Mixup    | -->\n","\n","**HSV Augmentation**: HSV (Hue, Saturation, Value) is a colour space developed by A. R. Smith in 1978 based on intuitive colour properties, often known as the Hexcone Model. This model’s colour parameters are hue (H), saturation (S), and lightness (V).\n","\n","`hsv_h`: HSV-Hue augmentation (fraction; value range 0.0 to 1.0)\\\n","`hsv_s`: HSV-Saturation augmentation (fraction; value range 0.0 to 1.0)\\\n","`hsv_v`: HSV-Value augmentation (fraction; value range 0.0 to 1.0)\n","\n","**Rotation (degrees) Augmentation**: A random rotation of the image clockwise or counterclockwise by a specified amount of degrees alters the item's location in the frame.\n","\n","`degrees`: rotation (+/- deg; value range -360.0 to 360.0)\n","\n","**Translation Augmentation**: Shifts the image along the x- and y-axis independently\n","\n","`translate`: translation (+/- fraction; value range -1.0 to 1.0)\n","\n","**Scale Augmentation**: Image is zoomed in or zoomed out\n","\n","`scale`: scale (+/- gain; value range -1.0 to 1.0)\n","\n","**Shear Augmentation**: Distort the image along the x- and y-axis to create new perception angles\n","\n","`shear`: shear (+/- deg; value range -360.0 to 360.0)\n","\n","**Perspective Augmentation**: Same as shear augmentation\n","\n","`perspective`: perspective (+/- fraction; value range 0-0.001; recommended to use 0.001)\n","\n","**Flip Augmentation**: Flip the image vertical (up/down) or horizontally (left/right)\n","\n","`flipud`: flip up-down (probability; value range 0.0 to 1.0)\\\n","`fliplr`: flip left-right (probability; value range 0.0 to 1.0)\n","\n","**Mosaic Augmentation**: Combine 4 images into one using a ratio\n","\n","`mosaic`: mosaic (probability; value range 0.0 to 1.0)\n","\n","**Mixup Augmentation**: Overlaying of 2 images and their labels\n","\n","`mixup`: mixup (probability; value range 0.0 to 1.0)"]},{"cell_type":"markdown","metadata":{"id":"1QIYtv8PQwzp"},"source":["### Sample.zip setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uvywotyTQ-ug"},"outputs":[],"source":["# YOLOv5 🚀 by Ultralytics, GPL-3.0 license\n","# COCO128 dataset https://www.kaggle.com/ultralytics/coco128 (first 128 images from COCO train2017) by Ultralytics\n","# Example usage: python train.py --data coco128.yaml\n","# parent\n","# ├── yolov5\n","# └── datasets\n","#     └── steamxd  ← downloads here (7 MB)\n","%%writetemplate /mydrive/steamxd/yolov5/data/{dataset_filename}.yaml\n","\n","# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\n","path: /mydrive/steamxd/datasets/{dataset_filename}  # dataset root dir\n","train: train/images  # train images (relative to 'path') 128 images\n","val: train/images  # val images (relative to 'path') 128 images\n","test:  # test images (optional)\n","\n","# Classes\n","names:\n","  0: humans\n","  1: pets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_RklsIbRt40"},"outputs":[],"source":["##### this is for medium model #####\n","%%writetemplate /mydrive/steamxd/yolov5/models/{dataset_filename}.yaml\n","\n","# Parameters\n","nc: 2  # number of classes\n","depth_multiple: 0.67  # model depth multiple\n","width_multiple: 0.75  # layer channel multiple\n","anchors:\n","  - [10,13, 16,30, 33,23]  # P3/8\n","  - [30,61, 62,45, 59,119]  # P4/16\n","  - [116,90, 156,198, 373,326]  # P5/32\n","\n","# YOLOv5 v6.0 backbone\n","backbone:\n","  # [from, number, module, args]\n","  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n","   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n","   [-1, 3, C3, [128]],\n","   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n","   [-1, 6, C3, [256]],\n","   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n","   [-1, 9, C3, [512]],\n","   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n","   [-1, 3, C3, [1024]],\n","   [-1, 1, SPPF, [1024, 5]],  # 9\n","  ]\n","\n","# YOLOv5 v6.0 head\n","head:\n","  [[-1, 1, Conv, [512, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n","   [-1, 3, C3, [512, False]],  # 13\n","\n","   [-1, 1, Conv, [256, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n","   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n","\n","   [-1, 1, Conv, [256, 3, 2]],\n","   [[-1, 14], 1, Concat, [1]],  # cat head P4\n","   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n","\n","   [-1, 1, Conv, [512, 3, 2]],\n","   [[-1, 10], 1, Concat, [1]],  # cat head P5\n","   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n","\n","   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n","  ]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%cd /mydrive/steamxd/datasets/{dataset_filename}/\n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!unzip ./{dataset_filename}.zip\n","%ls"]},{"cell_type":"markdown","metadata":{"id":"K7jsL0SLRwyT"},"source":["### Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3UvZbPKyawu5","scrolled":true},"outputs":[],"source":["# how to rewrite the hyps yaml\n","%%writetemplate /mydrive/steamxd/yolov5/data/hyps/hyp.scratch-low.yaml\n","\n","# YOLOv5 🚀 by Ultralytics, GPL-3.0 license\n","# Hyperparameters for low-augmentation COCO training from scratch\n","# python train.py --batch 64 --cfg yolov5n6.yaml --weights '' --data coco.yaml --img 640 --epochs 300 --linear\n","# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n","\n","#################################\n","### DO NOT ALTER THIS PORTION ###\n","#################################\n","lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\n","lrf: 0.01  # final OneCycleLR learning rate (lr0 * lrf)\n","momentum: 0.937  # SGD momentum/Adam beta1\n","weight_decay: 0.0005  # optimizer weight decay 5e-4\n","warmup_epochs: 3.0  # warmup epochs (fractions ok)\n","warmup_momentum: 0.8  # warmup initial momentum\n","warmup_bias_lr: 0.1  # warmup initial bias lr\n","box: 0.05  # box loss gain\n","cls: 0.5  # cls loss gain\n","cls_pw: 1.0  # cls BCELoss positive_weight\n","obj: 1.0  # obj loss gain (scale with pixels)\n","obj_pw: 1.0  # obj BCELoss positive_weight\n","iou_t: 0.20  # IoU training threshold\n","anchor_t: 4.0  # anchor-multiple threshold\n","# anchors: 3  # anchors per output layer (0 to ignore)\n","fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n","#################################\n","### DO NOT ALTER THIS PORTION ###\n","#################################\n","###########################################\n","### Image Augmentation Parameters Start ###\n","###########################################\n","hsv_h: {hsv_h}  # image HSV-Hue augmentation (fraction)\n","hsv_s: {hsv_s}  # image HSV-Saturation augmentation (fraction)\n","hsv_v: {hsv_v}  # image HSV-Value augmentation (fraction)\n","degrees: {degrees}  # image rotation (+/- deg)\n","translate: {translate}  # image translation (+/- fraction)\n","scale: {scale}  # image scale (+/- gain)\n","shear: {shear} # image shear (+/- deg)\n","perspective: {perspective}  # image perspective (+/- fraction), range 0-0.001\n","flipud: {flipud}  # image flip up-down (probability)\n","fliplr: {fliplr}  # image flip left-right (probability)\n","mosaic: {mosaic}  # image mosaic (probability)\n","mixup: {mixup}  # image mixup (probability)\n","#########################################\n","### Image Augmentation Parameters End ###\n","#########################################\n","# copy_paste is for image segmentation leave this augmentation alone\n","copy_paste: 0.0  # segment copy-paste (probability)"]},{"cell_type":"markdown","metadata":{"id":"9TFvXObkawu6"},"source":["## Training Parameters\n","\n","`--img`: to specific the input image size 640x640 \\\n","`--batch`: the batch size of the image example input (16 images) [further explaination](https://developers.google.com/machine-learning/glossary#batch) \\\n","`--epochs`: A full training pass over the entire training set such that each example has been processed once. [further explaination](https://developers.google.com/machine-learning/glossary#epoch) \\\n","`--data`: configuration file for where the image dataset is stored \\\n","`--weights`: pre-trained weights \\\n","`--cache`: store the training images in DRAM \\\n","`--name`: this is the folder name for different training runs (e.g. aug-mosaic, aug-mixup, etc) \\\n","`--project`: this is the directory path that you want to save your model runs in (e.g. '/mydrive/SUTD/STEAMxD') \\\n","`--resume`: this will resume training of your model if training is interrupted (e.g. epochs 100, training stops at epochs 55.) If all epochs has been done (100/100) but you still wish to continue training on the existing model use the `--weights` flag instead (e.g. --weights /mydrive/SUTD/STEAMxD/yolov5/runs/train/exp) \n","\n","<!---\n","--project can be removed if I'm gonna just be cloning the yolov5 repo into gdrive\n","--resume flag to resume training if you're initial training epochs (300 and your model stops as 150)\n","--weights alternatively you can use the --weights flag to specific the weights file to continue training from\n","--data I need to add in the yaml file edit so that I can link it to the correct dataset\n","I should add in and image to explain the epochs and batch size\n","-->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hffkm41OR4W5"},"outputs":[],"source":["# ensure that you're in the parent yolov5 folder\n","%cd /mydrive/steamxd/yolov5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mR1bu-MTawu7","scrolled":true},"outputs":[],"source":["# give some explanation of the different parameters\n","!python train.py --img 640 --batch 16 --epochs 1 --data ./data/{dataset_filename}.yaml --cfg ./models/{dataset_filename}.yaml --weights yolov5s.pt --cache --name {dataset_filename} --project '/mydrive/steamxd/yolov5/runs/train'"]},{"cell_type":"markdown","metadata":{"id":"xO60DlHIawu8"},"source":["## Visualising augmented images during training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CPNZpdQMawu8","scrolled":true},"outputs":[],"source":["# print out an augmented training example\n","print(\"GROUND TRUTH AUGMENTED TRAINING DATA:\")\n","Image(filename='/mydrive/steamxd/yolov5/runs/train/sample/train_batch0.jpg', width=900)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["7mGmQbAO5pQb","ftcLJthqawuw","RdePlB7GbAHY","1QIYtv8PQwzp","aKCHJbQga8MU","pT_D8USKcbq-","5pOSx2SqbwOi","GLyPofyrawu9"],"machine_shape":"hm","provenance":[{"file_id":"1wG77y_WXlRBl29FSoVg_YtpHCnqgPXY0","timestamp":1673700510841},{"file_id":"https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb","timestamp":1662297741718}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1 (tags/v3.8.1:1b293b6, Dec 18 2019, 23:11:46) [MSC v.1916 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"fd9a3f95d60962a936c2f57e8e13ba0f0dd2b320c32841c95743504c68ce9fb6"}}},"nbformat":4,"nbformat_minor":0}
