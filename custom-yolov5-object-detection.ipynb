{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6MPjfT5NrKQ"
   },
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "  <a href=\"https://ultralytics.com/yolov5\" target=\"_blank\">\n",
    "    <img width=\"1024\", src=\"https://github.com/ultralytics/assets/raw/master/yolov5/v62/splash_notebook.png\"></a>\n",
    "\n",
    "\n",
    "<br>\n",
    "  <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
    "<br>\n",
    "\n",
    "This <a href=\"https://github.com/ultralytics/yolov5\">YOLOv5</a> 🚀 notebook by ROAR Lab presents simple train, validate and predict examples to help start your AI adventure.<br>Credit of this notebook goes to <a href=\"https://ultralytics.com/\">Ultralytics</a> and <a href=\"https://roboflow.com/\">Roboflow</a> as this notebook used both sample as reference.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to Edit ##\n",
    "- To change the background of the YOLOv5 logo as well as the link buttons to STEAMxD\n",
    "- I need to include steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mGmQbAO5pQb"
   },
   "source": [
    "# First time setup\n",
    "\n",
    "Use this section to setup your GDrive for YOLOv5 usage with google colab for the first time. **Once this initial setup is done for further usage you will not be required to download the YOLOv5 repository again. Please use the next section [here](#future-setup) when accessing this colab notebook for future usage.**\n",
    "\n",
    "In this section we're mounting GDrive, cloning GitHub [repository](https://github.com/ultralytics/yolov5), installing [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt), importing necessary libraries and checking PyTorch and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# this creates a symbolic link so that now the path /content/gdrive/My Drive/ is equal to /mydrive\n",
    "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
    "%cd /mydrive\n",
    "!mkdir steamxd\n",
    "%cd steamxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5  # clone\n",
    "%cd yolov5\n",
    "%pip install -qr requirements.txt  # install\n",
    "\n",
    "import torch\n",
    "import utils\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "display = utils.notebook_init()  # checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mGmQbAO5pQb"
   },
   "source": [
    "# Future usage setup <a id=\"future-setup\"></a>\n",
    "\n",
    "If you already have YOLOv5 setup in your GDrive use this section to setup your colab notebook.\n",
    "\n",
    "In this section we're mounting GDrive, installing [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt), importing necessary libraries and checking PyTorch and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# this creates a symbolic link so that now the path /content/gdrive/My Drive/ is equal to /mydrive\n",
    "!ln -s /content/gdrive/My\\ Drive/ /mydrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbvMlHd_QwMG",
    "outputId": "0f9ee467-cea4-48e8-9050-7a76ae1b6141",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v6.2-56-g30e674b Python-3.7.13 torch-1.12.1+cu113 CUDA:0 (Tesla V100-SXM2-16GB, 16160MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete ✅ (8 CPUs, 51.0 GB RAM, 37.4/166.8 GB disk)\n"
     ]
    }
   ],
   "source": [
    "%cd /mydrive/steamxd/yolov5\n",
    "%pip install -qr requirements.txt  # install\n",
    "\n",
    "import torch\n",
    "import utils\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "display = utils.notebook_init()  # checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JnkELT0cIJg"
   },
   "source": [
    "# 1. Image Augmentation <a id=\"image-aug\"></a>\n",
    "\n",
    "In this example we will do a single augmentation (mosaic) and train the model for a single epoch to showcase the available augmentations while training.\n",
    "\n",
    "<!---\n",
    "To edit the instructions to include Augmentation discussions for ease of understanding\n",
    "\n",
    "\n",
    "`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n",
    "\n",
    "```shell\n",
    "python detect.py --source 0  # webcam\n",
    "                          img.jpg  # image \n",
    "                          vid.mp4  # video\n",
    "                          path/  # directory\n",
    "                          'path/*.jpg'  # glob\n",
    "                          'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n",
    "                          'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
    "```\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customize iPython writefile so we can write variables\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation Parameters\n",
    "\n",
    "- To add some simple explanation on the augmentation done\n",
    "\n",
    "| | Image Augmentation Examples | |\n",
    "| :-: | :-: | :-: |\n",
    "| <div><img src=\"https://user-images.githubusercontent.com/65292018/195560245-3d7a7b5e-13b0-4880-8116-339d50128723.jpg\" width=\"250\"/></div> | <div><img src=\"https://user-images.githubusercontent.com/65292018/195560259-988483e1-7ae1-476b-85c4-fccc35dd6741.jpg\" width=\"250\"/></div> | <div><img src=\"https://user-images.githubusercontent.com/65292018/195560275-87b11837-1891-4446-b06b-9ee33bf57357.jpg\" width=\"250\"/></div> |\n",
    "| HSV-Hue    | HSV-Saturation    | HSV-Value |\n",
    "| <div><img src=\"https://user-images.githubusercontent.com/65292018/195560226-674913b9-3042-4910-bcce-976ea12393b9.jpg\" width=\"250\"/></div> | <div><img src=\"https://user-images.githubusercontent.com/65292018/195560269-562f7e81-e6d3-413b-89f8-3d3dcecaf3b5.jpg\" width=\"250\"/></div> | <div><img src=\"https://user-images.githubusercontent.com/65292018/195560262-7eae030e-0d10-4b81-ab2f-0c18e26303b5.jpg\" width=\"250\"/></div> |\n",
    "| Rotation   | Translation       | Scale     |\n",
    "| <div><img src=\"https://user-images.githubusercontent.com/65292018/195560265-3e6d1ecf-142b-4ce4-9024-30f1a0b31159.jpg\" width=\"250\"/></div> | <div><img src=\"https://user-images.githubusercontent.com/65292018/195560255-8737528e-6c9a-4292-9381-7776a43ae386.jpg\" width=\"250\"/></div> | <div><img src=\"https://user-images.githubusercontent.com/65292018/195560237-9003d9af-572e-496e-896e-b3a471a6c3cf.jpg\" width=\"250\"/></div> |\n",
    "| Shear      | Perspective       | Flip (up/down) |\n",
    "| <div><img src=\"https://user-images.githubusercontent.com/65292018/195560232-12feddfe-c06d-4e0e-ba7c-528a7731ee79.jpg\" width=\"250\"/></div> | <div><img src=\"https://user-images.githubusercontent.com/65292018/195560251-3a13aa0f-4b95-4a1f-9da8-f6cc69ec812c.jpg\" width=\"250\"/></div> | <div><img src=\"https://user-images.githubusercontent.com/65292018/195560250-c1656e46-9fc9-42f6-a97f-d5368b5650fb.jpg\" width=\"250\"/></div> |\n",
    "| Flip (left/right)   | Mosaic   | Mixup    |\n",
    "\n",
    "**HSV Augmentation**: HSV (Hue, Saturation, Value) is a colour space developed by A. R. Smith in 1978 based on intuitive colour properties, often known as the Hexcone Model. This model’s colour parameters are hue (H), saturation (S), and lightness (V).\n",
    "\n",
    "`hsv_h`: HSV-Hue augmentation (fraction; value range 0.0 to 1.0)\\\n",
    "`hsv_s`: HSV-Saturation augmentation (fraction; value range 0.0 to 1.0)\\\n",
    "`hsv_v`: HSV-Value augmentation (fraction; value range 0.0 to 1.0)\n",
    "\n",
    "**Rotation (degrees) Augmentation**: A random rotation of the image clockwise or counterclockwise by a specified amount of degrees alters the item's location in the frame.\n",
    "\n",
    "`degrees`: rotation (+/- deg; value range -360.0 to 360.0)\n",
    "\n",
    "**Translation Augmentation**: Shifts the image along the x- and y-axis independently\n",
    "\n",
    "`translate`: translation (+/- fraction; value range -1.0 to 1.0)\n",
    "\n",
    "**Scale Augmentation**: Image is zoomed in or zoomed out\n",
    "\n",
    "`scale`: scale (+/- gain; value range -1.0 to 1.0)\n",
    "\n",
    "**Shear Augmentation**: Distort the image along the x- and y-axis to create new perception angles\n",
    "\n",
    "`shear`: shear (+/- deg; value range -360.0 to 360.0)\n",
    "\n",
    "**Perspective Augmentation**: Same as shear augmentation\n",
    "\n",
    "`perspective`: perspective (+/- fraction; value range 0-0.001; recommended to use 0.001)\n",
    "\n",
    "**Flip Augmentation**: Flip the image vertical (up/down) or horizontally (left/right)\n",
    "\n",
    "`flipud`: flip up-down (probability; value range 0.0 to 1.0)\\\n",
    "`fliplr`: flip left-right (probability; value range 0.0 to 1.0)\n",
    "\n",
    "**Mosaic Augmentation**: Combine 4 images into one using a ratio\n",
    "\n",
    "`mosaic`: mosaic (probability; value range 0.0 to 1.0)\n",
    "\n",
    "**Mixup Augmentation**: Overlaying of 2 images and their labels\n",
    "\n",
    "`mixup`: mixup (probability; value range 0.0 to 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%writetemplate` not found.\n"
     ]
    }
   ],
   "source": [
    "# how to rewrite the hyps yaml\n",
    "%%writetemplate /mydrive/steamxd/yolov5/data/hyps/hyp.scratch-low.yaml\n",
    "\n",
    "# YOLOv5 🚀 by Ultralytics, GPL-3.0 license\n",
    "# Hyperparameters for low-augmentation COCO training from scratch\n",
    "# python train.py --batch 64 --cfg yolov5n6.yaml --weights '' --data coco.yaml --img 640 --epochs 300 --linear\n",
    "# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n",
    "\n",
    "#################################\n",
    "### DO NOT ALTER THIS PORTION ###\n",
    "#################################\n",
    "lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "lrf: 0.01  # final OneCycleLR learning rate (lr0 * lrf)\n",
    "momentum: 0.937  # SGD momentum/Adam beta1\n",
    "weight_decay: 0.0005  # optimizer weight decay 5e-4\n",
    "warmup_epochs: 3.0  # warmup epochs (fractions ok)\n",
    "warmup_momentum: 0.8  # warmup initial momentum\n",
    "warmup_bias_lr: 0.1  # warmup initial bias lr\n",
    "box: 0.05  # box loss gain\n",
    "cls: 0.5  # cls loss gain\n",
    "cls_pw: 1.0  # cls BCELoss positive_weight\n",
    "obj: 1.0  # obj loss gain (scale with pixels)\n",
    "obj_pw: 1.0  # obj BCELoss positive_weight\n",
    "iou_t: 0.20  # IoU training threshold\n",
    "anchor_t: 4.0  # anchor-multiple threshold\n",
    "# anchors: 3  # anchors per output layer (0 to ignore)\n",
    "fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n",
    "#################################\n",
    "### DO NOT ALTER THIS PORTION ###\n",
    "#################################\n",
    "###########################################\n",
    "### Image Augmentation Parameters Start ###\n",
    "###########################################\n",
    "hsv_h: 0.0  # image HSV-Hue augmentation (fraction)\n",
    "hsv_s: 0.0  # image HSV-Saturation augmentation (fraction)\n",
    "hsv_v: 0.0  # image HSV-Value augmentation (fraction)\n",
    "degrees: 0.0  # image rotation (+/- deg)\n",
    "translate: 0.0  # image translation (+/- fraction)\n",
    "scale: 0.0  # image scale (+/- gain)\n",
    "shear: 0.0  # image shear (+/- deg)\n",
    "perspective: 0.000  # image perspective (+/- fraction), range 0-0.001\n",
    "flipud: 0.0  # image flip up-down (probability)\n",
    "fliplr: 0.0  # image flip left-right (probability)\n",
    "mosaic: 1.0  # image mosaic (probability)\n",
    "mixup: 0.0  # image mixup (probability)\n",
    "#########################################\n",
    "### Image Augmentation Parameters End ###\n",
    "#########################################\n",
    "# copy_paste is for image segmentation leave this augmentation alone\n",
    "copy_paste: 0.0  # segment copy-paste (probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters\n",
    "\n",
    "- Find the explaination from train.py and then include more verbose explaination\n",
    "\n",
    "`--img`: to specific the input image size 640x640 \\\n",
    "`--batch`: the batch size of the image example input (16 images) [further explaination](https://developers.google.com/machine-learning/glossary#batch) \\\n",
    "`--epochs`: A full training pass over the entire training set such that each example has been processed once. [further explaination](https://developers.google.com/machine-learning/glossary#epoch) \\\n",
    "`--data`: configuration file for where the image dataset is stored \\\n",
    "`--weights`: pre-trained weights \\\n",
    "`--cache`: \\\n",
    "`--name`: this is the folder name for different training runs (e.g. aug-mosaic, aug-mixup, etc) \\\n",
    "`--project`: this is the directory path that you want to save your model runs in (e.g. '/mydrive/SUTD/STEAMxD') \\\n",
    "`--resume`: this will resume training of your model if training is interrupted (e.g. epochs 100, training stops at epochs 55.) If all epochs has been done (100/100) but you still wish to continue training on the existing model use the `--weights` flag instead (e.g. --weights /mydrive/SUTD/STEAMxD/yolov5/runs/train/exp) \n",
    "\n",
    "<!---\n",
    "--project can be removed if I'm gonna just be cloning the yolov5 repo into gdrive\n",
    "--resume flag to resume training if you're initial training epochs (300 and your model stops as 150)\n",
    "--weights alternatively you can use the --weights flag to specific the weights file to continue training from\n",
    "--data I need to add in the yaml file edit so that I can link it to the correct dataset\n",
    "I should add in and image to explain the epochs and batch size\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give some explanation of the different parameters\n",
    "!python train.py --img 640 --batch 16 --epochs 1 --data coco128.yaml --weights yolov5s.pt --cache --name aug-mosaic --project '/mydrive/SUTD/STEAMxD'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "- to insert description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard\n",
    "# Launch after you have started training\n",
    "# logs save in the folder \"runs\"\n",
    "# if you see a google 403 error please ensure that your browser allows 3rd party cookie for tensorboard to be viewable\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising augmented images during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out an augmented training example\n",
    "print(\"GROUND TRUTH AUGMENTED TRAINING DATA:\")\n",
    "Image(filename='/content/yolov5/runs/train/aug-mosaic2/train_batch1.jpg', width=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JnkELT0cIJg"
   },
   "source": [
    "# 2. Training custom model\n",
    "\n",
    "Using the augmentation parameters that you've experimented in the [previous section](#image-aug), we will be training the custom model using your custom dataset.\n",
    "\n",
    "<!---\n",
    "To edit the instructions to include Augmentation discussions for ease of understanding\n",
    "\n",
    "\n",
    "`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n",
    "\n",
    "```shell\n",
    "python detect.py --source 0  # webcam\n",
    "                          img.jpg  # image \n",
    "                          vid.mp4  # video\n",
    "                          path/  # directory\n",
    "                          'path/*.jpg'  # glob\n",
    "                          'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n",
    "                          'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
    "```\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%writetemplate` not found.\n"
     ]
    }
   ],
   "source": [
    "# how to rewrite the hyps yaml\n",
    "%%writetemplate /mydrive/steamxd/yolov5/data/hyps/hyp.scratch-low.yaml\n",
    "\n",
    "# YOLOv5 🚀 by Ultralytics, GPL-3.0 license\n",
    "# Hyperparameters for low-augmentation COCO training from scratch\n",
    "# python train.py --batch 64 --cfg yolov5n6.yaml --weights '' --data coco.yaml --img 640 --epochs 300 --linear\n",
    "# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n",
    "\n",
    "#################################\n",
    "### DO NOT ALTER THIS PORTION ###\n",
    "#################################\n",
    "lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "lrf: 0.01  # final OneCycleLR learning rate (lr0 * lrf)\n",
    "momentum: 0.937  # SGD momentum/Adam beta1\n",
    "weight_decay: 0.0005  # optimizer weight decay 5e-4\n",
    "warmup_epochs: 3.0  # warmup epochs (fractions ok)\n",
    "warmup_momentum: 0.8  # warmup initial momentum\n",
    "warmup_bias_lr: 0.1  # warmup initial bias lr\n",
    "box: 0.05  # box loss gain\n",
    "cls: 0.5  # cls loss gain\n",
    "cls_pw: 1.0  # cls BCELoss positive_weight\n",
    "obj: 1.0  # obj loss gain (scale with pixels)\n",
    "obj_pw: 1.0  # obj BCELoss positive_weight\n",
    "iou_t: 0.20  # IoU training threshold\n",
    "anchor_t: 4.0  # anchor-multiple threshold\n",
    "# anchors: 3  # anchors per output layer (0 to ignore)\n",
    "fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n",
    "#################################\n",
    "### DO NOT ALTER THIS PORTION ###\n",
    "#################################\n",
    "###########################################\n",
    "### Image Augmentation Parameters Start ###\n",
    "###########################################\n",
    "hsv_h: 0.0  # image HSV-Hue augmentation (fraction)\n",
    "hsv_s: 0.0  # image HSV-Saturation augmentation (fraction)\n",
    "hsv_v: 0.0  # image HSV-Value augmentation (fraction)\n",
    "degrees: 0.0  # image rotation (+/- deg)\n",
    "translate: 0.0  # image translation (+/- fraction)\n",
    "scale: 0.0  # image scale (+/- gain)\n",
    "shear: 0.0  # image shear (+/- deg)\n",
    "perspective: 0.000  # image perspective (+/- fraction), range 0-0.001\n",
    "flipud: 0.0  # image flip up-down (probability)\n",
    "fliplr: 0.0  # image flip left-right (probability)\n",
    "mosaic: 1.0  # image mosaic (probability)\n",
    "mixup: 0.0  # image mixup (probability)\n",
    "#########################################\n",
    "### Image Augmentation Parameters End ###\n",
    "#########################################\n",
    "# copy_paste is for image segmentation leave this augmentation alone\n",
    "copy_paste: 0.0  # segment copy-paste (probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Configuration and Architecture\n",
    "\n",
    "We will write a yaml script that defines the parameters for our model like the number of classes, anchors, and each layer.\n",
    "\n",
    "You do not need to edit these cells, but you may."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv5 🚀 by Ultralytics, GPL-3.0 license\n",
    "# COCO128 dataset https://www.kaggle.com/ultralytics/coco128 (first 128 images from COCO train2017) by Ultralytics\n",
    "# Example usage: python train.py --data coco128.yaml\n",
    "# parent\n",
    "# ├── yolov5\n",
    "# └── datasets\n",
    "#     └── coco128  ← downloads here (7 MB)\n",
    "\n",
    "\n",
    "# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\n",
    "path: ../datasets/coco128  # dataset root dir\n",
    "train: images/train2017  # train images (relative to 'path') 128 images\n",
    "val: images/train2017  # val images (relative to 'path') 128 images\n",
    "test:  # test images (optional)\n",
    "\n",
    "# Classes\n",
    "names:\n",
    "  0: person\n",
    "  1: bicycle\n",
    "  2: car\n",
    "  3: motorcycle\n",
    "  4: airplane\n",
    "  5: bus\n",
    "  6: train\n",
    "  7: truck\n",
    "  8: boat\n",
    "  9: traffic light\n",
    "  10: fire hydrant\n",
    "  11: stop sign\n",
    "  12: parking meter\n",
    "  13: bench\n",
    "  14: bird\n",
    "  15: cat\n",
    "  16: dog\n",
    "  17: horse\n",
    "  18: sheep\n",
    "  19: cow\n",
    "  20: elephant\n",
    "  21: bear\n",
    "  22: zebra\n",
    "  23: giraffe\n",
    "  24: backpack\n",
    "  25: umbrella\n",
    "  26: handbag\n",
    "  27: tie\n",
    "  28: suitcase\n",
    "  29: frisbee\n",
    "  30: skis\n",
    "  31: snowboard\n",
    "  32: sports ball\n",
    "  33: kite\n",
    "  34: baseball bat\n",
    "  35: baseball glove\n",
    "  36: skateboard\n",
    "  37: surfboard\n",
    "  38: tennis racket\n",
    "  39: bottle\n",
    "  40: wine glass\n",
    "  41: cup\n",
    "  42: fork\n",
    "  43: knife\n",
    "  44: spoon\n",
    "  45: bowl\n",
    "  46: banana\n",
    "  47: apple\n",
    "  48: sandwich\n",
    "  49: orange\n",
    "  50: broccoli\n",
    "  51: carrot\n",
    "  52: hot dog\n",
    "  53: pizza\n",
    "  54: donut\n",
    "  55: cake\n",
    "  56: chair\n",
    "  57: couch\n",
    "  58: potted plant\n",
    "  59: bed\n",
    "  60: dining table\n",
    "  61: toilet\n",
    "  62: tv\n",
    "  63: laptop\n",
    "  64: mouse\n",
    "  65: remote\n",
    "  66: keyboard\n",
    "  67: cell phone\n",
    "  68: microwave\n",
    "  69: oven\n",
    "  70: toaster\n",
    "  71: sink\n",
    "  72: refrigerator\n",
    "  73: book\n",
    "  74: clock\n",
    "  75: vase\n",
    "  76: scissors\n",
    "  77: teddy bear\n",
    "  78: hair drier\n",
    "  79: toothbrush\n",
    "\n",
    "\n",
    "# Download script/URL (optional)\n",
    "download: https://ultralytics.com/assets/coco128.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the model configuration we will use for our training\n",
    "%cat /content/yolov5/models/yolov5s.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate /content/yolov5/models/custom_yolov5s.yaml\n",
    "\n",
    "# parameters\n",
    "nc: {num_classes}  # number of classes\n",
    "depth_multiple: 0.33  # model depth multiple\n",
    "width_multiple: 0.50  # layer channel multiple\n",
    "\n",
    "# anchors\n",
    "anchors:\n",
    "  - [10,13, 16,30, 33,23]  # P3/8\n",
    "  - [30,61, 62,45, 59,119]  # P4/16\n",
    "  - [116,90, 156,198, 373,326]  # P5/32\n",
    "\n",
    "# YOLOv5 backbone\n",
    "backbone:\n",
    "  # [from, number, module, args]\n",
    "  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n",
    "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
    "   [-1, 3, BottleneckCSP, [128]],\n",
    "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
    "   [-1, 9, BottleneckCSP, [256]],\n",
    "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
    "   [-1, 9, BottleneckCSP, [512]],\n",
    "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
    "   [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
    "   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n",
    "  ]\n",
    "\n",
    "# YOLOv5 head\n",
    "head:\n",
    "  [[-1, 1, Conv, [512, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
    "   [-1, 3, BottleneckCSP, [512, False]],  # 13\n",
    "\n",
    "   [-1, 1, Conv, [256, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
    "   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n",
    "\n",
    "   [-1, 1, Conv, [256, 3, 2]],\n",
    "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
    "   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n",
    "\n",
    "   [-1, 1, Conv, [512, 3, 2]],\n",
    "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
    "   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n",
    "\n",
    "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters\n",
    "\n",
    "- Find the explaination from train.py and then include more verbose explaination\n",
    "\n",
    "`--img`: \\\n",
    "`--batch`: \\\n",
    "`--epochs`: \\\n",
    "`--data`: \\\n",
    "`--weights`: \\\n",
    "`--cache`: \\\n",
    "`--name`: \\\n",
    "`--project`: this is the directory path that you want to save your model runs in (e.g. '/mydrive/SUTD/STEAMxD')\\\n",
    "`--resume`: this will resume training of your model if training is interrupted\n",
    "\n",
    "<!---\n",
    "--project can be removed if I'm gonna just be cloning the yolov5 repo into gdrive\n",
    "--resume flag to resume training if you're initial training epochs (300 and your model stops as 150)\n",
    "--weights alternatively you can use the --weights flag to specific the weights file to continue training from\n",
    "--data I need to add in the yaml file edit so that I can link it to the correct dataset\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give some explanation of the different parameters\n",
    "!python train.py --img 640 --batch 16 --epochs 1 --data coco128.yaml --weights yolov5s.pt --cache --name aug-mosaic --project '/mydrive/SUTD/STEAMxD'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "- to insert description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard\n",
    "# Launch after you have started training\n",
    "# logs save in the folder \"runs\"\n",
    "# if you see a google 403 error please ensure that your browser allows 3rd party cookie for tensorboard to be viewable\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising augmented images during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out an augmented training example\n",
    "print(\"GROUND TRUTH AUGMENTED TRAINING DATA:\")\n",
    "Image(filename='/content/yolov5/runs/train/aug-mosaic2/train_batch1.jpg', width=900)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb",
     "timestamp": 1662297741718
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0856bea36ec148b68522ff9c9eb258d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0ace3934ec6f4d36a1b3a9e086390926": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "35e03ce5090346c9ae602891470fc555": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76879f6f2aa54637a7a07faeea2bd684",
      "max": 818322941,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0ace3934ec6f4d36a1b3a9e086390926",
      "value": 818322941
     }
    },
    "574140e4c4bc48c9a171541a02cd0211": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60b913d755b34d638478e30705a2dde1",
      "placeholder": "​",
      "style": "IPY_MODEL_0856bea36ec148b68522ff9c9eb258d8",
      "value": "100%"
     }
    },
    "5966ba6e6f114d8c9d8d1d6b1bd4f4c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "60b913d755b34d638478e30705a2dde1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65881db1db8a4e9c930fab9172d45143": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76879f6f2aa54637a7a07faeea2bd684": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b8caa3522fc4cbab31e13b5dfc7808d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_574140e4c4bc48c9a171541a02cd0211",
       "IPY_MODEL_35e03ce5090346c9ae602891470fc555",
       "IPY_MODEL_c942c208e72d46568b476bb0f2d75496"
      ],
      "layout": "IPY_MODEL_65881db1db8a4e9c930fab9172d45143"
     }
    },
    "c942c208e72d46568b476bb0f2d75496": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6b7a2243e0c4beca714d99dceec23d6",
      "placeholder": "​",
      "style": "IPY_MODEL_5966ba6e6f114d8c9d8d1d6b1bd4f4c7",
      "value": " 780M/780M [02:19&lt;00:00, 6.24MB/s]"
     }
    },
    "d6b7a2243e0c4beca714d99dceec23d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
